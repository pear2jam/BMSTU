% Лабораторная работа № 1.2. Лексический анализатор на основе регулярных выражений
% 29 апреля 2024 г.
% Наумов Сергей ИУ9-62Б

# Цель работы
Целью данной работы является приобретение навыка разработки простейших
лексических анализаторов, работающих на основе поиска в тексте по образцу,
заданному регулярным выражением.

# Реализация

## Регулярные выражения используемые для классификации токенов:

- **regex_integer**: `0x[0-9A-Fa-f]+|0t[0-7]+|0b[01]+|\d+`
  - Символы: Шестнадцатеричные числа (начинаются с "0x"), восьмеричные числа (начинаются с "0t"),
   двоичные числа (начинаются с "0b") или обычные десятичные числа.
  - Идентификатор: INTEGER

- **regex_keywords**: `and|or`
  - Символы: Ключевые слова "and" или "or".
  - Идентификатор: KEYWORD

- **regex_operators**: `\(|\)`
  - Символы: Открывающая или закрывающая скобка.
  - Идентификатор: OPERATOR

- **regex_identifiers**: `[a-zA-Z]+`
  - Символы: Последовательности латинских букв.
  - Идентификатор: IDENTIFIER

## Описание работы

- Функция `lex` выполняет лексический анализ входного текста, разбивая его на лексемы.-
-  Для каждой строки входного текста применяется регулярное выражение `regex_combined`,
   которое комбинирует все остальные регулярные выражения.
- Каждая лексема определяется с помощью соответствующего регулярного выражения,
   и создается объект `Token` с типом и данными лексемы.
- Созданные токены добавляются в список `tokens`.
- После завершения лексического анализа список токенов выводится на экран.


## Код реализации
```python
import re

class Token:
    def __init__(self, token_type, data, line_number, column_number):
        self.token_type = token_type
        self.data = data
        self.line_number = line_number
        self.column_number = column_number
    
    def __str__(self):
        return f"{self.token_type} ({self.line_number}, {self.column_number}): {self.data}"

def lex(input_lines):
    tokens = []

    regex_integer = r"0x[0-9A-Fa-f]+|0t[0-7]+|0b[01]+|\d+"
    regex_keywords = r"and|or"
    regex_operators = r"\(|\)"
    regex_identifiers = r"[a-zA-Z]+"

    regex_combined = f"{regex_integer}|{regex_keywords}|{regex_operators}|{regex_identifiers}"

    for line_number, line in enumerate(input_lines, start=1):
        tokens_data = re.split(f"({regex_combined})", line.strip())
        current_column_number = 1

        for token_data in tokens_data:
            if token_data:
                if re.match(regex_integer, token_data):
                    token_type = "INTEGER"
                elif re.match(regex_keywords, token_data):
                    token_type = "KEYWORD"
                elif re.match(regex_operators, token_data):
                    token_type = "OPERATOR"
                elif re.match(regex_identifiers, token_data):
                    token_type = "IDENTIFIER"
                else:
                    token_type = "ERROR"

                if token_data.strip():
                    tokens.append(Token(token_type, token_data, line_number, current_column_number))
                    current_column_number += len(token_data)

    return tokens

file_path = "test.txt"
with open(file_path, "r") as file:
    input_lines = file.readlines()

tokens = lex(input_lines)
for token in tokens:
    print(token)

```

# Тестирование

Протестируем программу на таком примере:
```
and (0x1F and 0b1010) or (x and y) (100)
0b1011 !!!!!!! 22  2+3
```

Получим вывод:
```
INTEGER (1, 5): 0x1F
KEYWORD (1, 9): and
INTEGER (1, 12): 0b1010
OPERATOR (1, 18): )
KEYWORD (1, 19): or
OPERATOR (1, 21): (
KEYWORD (1, 23): and
IDENTIFIER (1, 26): y
OPERATOR (1, 27): )
OPERATOR (1, 28): (
INTEGER (1, 29): 100
OPERATOR (1, 32): )
INTEGER (2, 1): 0b1011
ERROR (2, 7):  !!!!!!!
INTEGER (2, 16): 22
INTEGER (2, 18): 2
ERROR (2, 19): +
INTEGER (2, 20): 3
```

Этот вывод корректный

# Вывод
В ходе выполнения лабораторной работы я разработал простой лексический анализатор,
основанный на использовании регулярных выражений для классификации лексем.
Мне удалось изучить основные принципы разработки лексических анализаторов
и применение регулярных выражений для их создания.

Я обнаружил следущие полезные свойства такого подхода к лексическому анализу:
- Эффективность: Регулярные выражения позволяют компактно описать шаблоны лексем,
   что делает процесс лексического анализа быстрым и эффективным.
- Гибкость: Благодаря возможности использования различных конструкций регулярных
  выражений, можно легко определить сложные шаблоны лексем, такие как числа
  разных систем счисления, идентификаторы, ключевые слова и операторы.
- Масштабируемость: Регулярные выражения могут быть легко модифицированы и
  расширены для обработки новых типов лексем или изменения лексических правил.
- Удобство отладки: При использовании регулярных выражений для лексического
  анализа можно легко проверить правильность шаблонов на примерах входных
  данных, что упрощает процесс отладки.
- Портативность: Код, основанный на регулярных выражениях, легко переносится
   между различными языками программирования, что делает реализацию лексического
   анализатора на основе регулярных выражений универсальным и портативным решением.

Этот способ лексического анализа является эффективным и удобным для первого этапа
компиляции программы. Он позволяет мне быстро и точно разбивать входной текст на лексемы,
что облегчает последующие этапы анализа и синтаксического разбора. Это особенно полезно 
для дальнейших лабораторных работ, в которых нужно работать с последущими этапами компиляции
на основе полученных лексем, в рамках которых этот способ представляет прозрачную 
и легкореализумую основу
